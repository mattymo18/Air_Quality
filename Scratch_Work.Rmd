---
title: "Scratch Work"
author: "Matt Johnson"
date: "10/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

Read in data
```{r}
Raleigh_Air <- read.csv("Source_Data/Raleigh_Air.csv") %>% 
  select(city, utc, parameter, value, latitude, longitude)
Greenville_Air <- read.csv("Source_Data/Greenville_Air.csv") %>% 
  select(city, utc, parameter, value, latitude, longitude)
```

```{r}
Raleigh <- spread(Raleigh_Air, key = parameter, value = value)

Raleigh$utc = as.Date(Raleigh$utc) 
Raleigh$Time <- format(Raleigh$utc,"%H:%M:%S")
Raleigh = Raleigh %>% 
  select(-Time)
Raleigh.Clean <- Raleigh %>% 
  group_by(utc, .drop = T) %>% 
  summarise(CO = mean(co), 
            NO2 = mean(no2), 
            O3 = mean(o3), 
            PM10 = mean(pm10), 
            PM25 = mean(pm25), 
            SO2 = mean(so2)) %>% 
  mutate(Location = "Raleigh") %>% 
  arrange(utc) %>% 
  select(Location, utc, CO, NO2, O3, PM10, PM25, SO2) #%>% 
  #filter(!is.na(PM25))

Greenville <- spread(Greenville_Air, key = parameter, value = value)

Greenville$utc = as.Date(Greenville$utc) 
Greenville$Time <- format(Greenville$utc,"%H:%M:%S")
Greenville = Greenville %>% 
  select(-Time)
Greenville.Clean <- Greenville %>% 
  group_by(utc, .drop = T) %>% 
  summarise(PM25 = mean(pm25)) %>% 
  mutate(Location = "Greenville") %>% 
  arrange(utc) %>% 
  select(Location, utc, PM25)
```

Akshay's data binding with covid and raleigh clean

```{r}
library(tidyverse)
library(lubridate)
```

```{r}
Confirmed_Cases <- read_csv("Source_Data/covid_confirmed_usafacts.csv")
Raleigh_Clean <- read_csv("derived_data/Raleigh.Clean.csv")
```

```{r}
#Begin_Date = gsub("^0", "", format(min(Raleigh_Clean$utc), format = "%m/%d/%Y"))
#End_Date = gsub("^0", "", format(max(Raleigh_Clean$utc), format = "%m/%d/%Y"))
Confirmed_Cases_Raleigh <- subset(Confirmed_Cases, State =="NC" & countyFIPS == 37183, select = `1/22/20`:`10/18/20`)

day <- seq(10,21)
for (i in day) {
Confirmed_Cases_Raleigh[[paste("1/", i,"/20",sep = "")]] = 0
}

Pivoted_Cases <- Confirmed_Cases_Raleigh %>%
  pivot_longer(cols = `1/22/20`:`1/21/20`,names_to = "utc", values_to = "Cases")

Pivoted_Cases$utc <- as.Date(Pivoted_Cases$utc, "%m/%d/%Y")
year(Pivoted_Cases$utc) <- 2020

Raleigh_Clean_COVID <- merge(Raleigh_Clean, Pivoted_Cases,by="utc")

write.csv("derived_data/Raleigh.Clean.Covid.csv")
```


Andy's Scratchwork

```{r}
#read in the data
Gdat<-read.csv("derived_data/Greenville.Clean.Covid.csv")
Rdat<-read.csv("derived_data/Raleigh.Clean.Covid.csv")
#initialize variables
t<-Gdat[,2]
gtime<-as.Date(t, "%Y-%m-%d")
Gpm<-Gdat[,5]
Gcase<-Gdat[,6]

d<-Rdat[,2]
rtime<-as.Date(d, "%Y-%m-%d")
Rpm<-Rdat[,9]
Rcase<-Rdat[,11]

#initial plots for assessing transformation needs and 
#model fits
plot(Gcase, Gpm, xlab= "Greenville Corona Case Count", ylab="Greenville Particulate Matter 2.5")
plot(gtime, Gpm, xlab="Time", ylab="Greenville Particulate Matter 2.5", type="o", col="blue")
plot(gtime, Gcase, xlab="Time", ylab="Greenville Corona Case Count", col="red", pch="*")

plot(Rcase, Rpm, xlab= "Raleigh Corona Case Count", ylab="Raleigh Particulate Matter 2.5")
plot(rtime, Rpm, xlab="Time", ylab="Raleigh Particulate Matter 2.5", type="o", col="green")
plot(rtime, Rcase, xlab="Time", ylab="Raleigh Corona Case Count", col="purple", pch="*")


```

ggplot version of above
```{r}
#switch some axis
Gdat<-read.csv("derived_data/Greenville.Clean.Covid.csv")
Rdat<-read.csv("derived_data/Raleigh.Clean.Covid.csv")


g1 <- Gdat %>% 
  ggplot(aes(x = Cases, y = PM25)) +
  geom_point(alpha = .5, color = "Red") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ggtitle("Raleigh") +
  theme(plot.title = element_text(hjust = 0.5))
g2 <- Rdat %>% 
  ggplot(aes(x = Cases, y = PM25)) +
  geom_point(alpha = .5, color = "Purple") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))  +
  ggtitle("Greenville") +
  theme(plot.title = element_text(hjust = 0.5))

Graph1 <- grid.arrange(g1, g2, nrow = 1)
ggsave("derived_graphs/PM25.Vs.Cases.plot.png", plot = Graph1)

g3 <-  Gdat %>% 
  ggplot(aes(x = as.Date(utc, "%Y-%m-%d"), y = PM25)) +
  geom_point(alpha = .5, color = "Purple") +
  geom_line(color = "Purple")  +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ggtitle("Greenville") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylab("Particulate Matter 2.5") +
  xlab("Date")

g4 <-  Rdat %>% 
  ggplot(aes(x = as.Date(utc, "%Y-%m-%d"), y = PM25)) +
  geom_point(alpha = .5, color = "Red") +
  geom_line(color = "Red")  +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ggtitle("Raleigh") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylab("Particulate Matter 2.5") +
  xlab("Date")

Graph2 <- grid.arrange(g3, g4, nrow = 1)
ggsave("derived_graphs/Time.Vs.PM25.plot.png", plot = Graph1)

g5 <- Gdat %>% 
  ggplot(aes(x = as.Date(utc, "%Y-%m-%d"), y = Cases)) +
  geom_point(alpha = .5, color = "Purple") +
  geom_line(color = "Purple")  +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ggtitle("Greenville") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylab("Particulate Matter 2.5") +
  xlab("Date")

g6 <- Rdat %>% 
  ggplot(aes(x = as.Date(utc, "%Y-%m-%d"), y = Cases)) +
  geom_point(alpha = .5, color = "Red") +
  geom_line(color = "Red")  +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ggtitle("Raleigh") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylab("Particulate Matter 2.5") +
  xlab("Date")

Graph3 <- grid.arrange(g6, g5, nrow = 1)
ggsave("derived_graphs/Time.Vs.Cases.plot.png", plot = Graph3)  
```





## Initial Reaction
With regard to both cities, I think it will be prudent to do some sort of variable transformation on the explanatory variable.  Right now, the trend appears almost lateral because of the discrepancy in scale.  That said, it is worth noting that while Corona case exhibit distinctly nonlinear trends with respect time time (coinciding with our intuition about the virus to date), their relationship with respect to PM does not seem to exhibit anything starkly non-linear.

As a second observation, I find this spike in PM (in both Raleigh and Greenville) curious.  This should be in the height of stay at home orders, so this seems counterintuitive.  Perhaps it is merely an outlier, but given its presence in both data sets, I think we would do well not to simply ignore it.  One potential explanation seems to be that since particulate matter includes particles from industries, perhaps coal burner power plants were straining under the demands of newly house-ridden customers and expelling more waste.  

The last and most stark observation is the hole in the Raleigh data.  I hadn't noticed this before, but it seems that quarantine also affected data collection.  While this is certainly not optimal (that was a rather crucial and informative time period) it can be alluded to in the paper, and I don't think it is completely damning.  


```{r}
## transformation consideration

sqcase<-sqrt(Gcase)
stripchart(Gdat[,6],method='jitter',las=2,vertical=TRUE, main="Strip Chart Greenville Corona Case Count")
boxplot(Gdat[,6], main="Box Plot of Greenville Corona Case Count")
boxplot(sqcase, main="Box Plot of Square Root Transformed Greenville Corona Case Count")
boxplot(Gdat[,5], main="Box Plot of Greenville Particulate Air Matter")
plot(sqcase, Gpm, xlab= "Square Root Tranformed Greenville Corona Case Count", ylab="Greenville Particulate Matter 2.5")


SQcase<-sqrt(Rcase)
stripchart(Rdat[,11],method='jitter',las=2,vertical=TRUE, main="Strip Chart Raleigh Corona Case Count")
boxplot(Rdat[,11], main="Box Plot of Raleigh Corona Case Count")
boxplot(SQcase, main="Box Plot of Square Root Transformed Raleigh Corona Case Count")
boxplot(Rdat[,9], main="Box Plot of Raleigh Particulate Air Matter")
plot(SQcase, Rpm, xlab= "Square Root Tranformed Raleigh Corona Case Count", ylab="Raleigh Particulate Matter 2.5")
```

ggplot versions of boxplots
```{r}
PM25.data <- rbind(Gdat[, c(4, 5)] , Rdat[, c(4, 9)])

g7 <- PM25.data %>% 
  ggplot(aes(y = PM25, x = Location)) +
  geom_boxplot(aes(color = Location)) + 
  geom_jitter(aes(color = Location), alpha = .5) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  ggtitle("Particulate Matter 2.5") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(breaks = c("Greenville", "Raleigh"), values=c("Purple", "Red")) 
ggsave("derived_graphs/PM25.boxplot.png", plot = g7)  
```


## Reflection on Transformation

Box plots elucidate the presence of outliers in Greenville Corona and PM data as well as Raleigh PM data.  A simple square root transform of the explanatory variable seems to ameliorate the problem of outliers and lessen the skewness in the Greenville corona data (though admittedly some remains).  For this reason, I think a square root transformation is justifiable.  Now, it is of note that the Raleigh corona data does not present the same outliers.  However, for compatibility of interpretation between cities I think it is wise to transform the Raleigh data in an analogous manner.  Also notice that while it may not eradicate outliers it does help to center the data a bit more.  Now, it could also be argued that we may need to transform the response variable to handle distortion.  This is fair.  My only hesitation is that often muddies the task of interpretation.  If necessary, I think it is fine to transform, but I wanted to get your take on it before I did anything with the response.  

```{r}
#initial model for Greenville

N<-length(t)
p<-2
#initialize time(tax) and observation (Y) arrays
tax<-1:N
Y<-matrix(0,N,1)
#define observations
Y<-sqcase
#define design matrix
D<-matrix(0,N,p)
for(i in 1:N){
  D[i,1]<-1
  D[1,2]<-i
}
#compute OLS Estimators
H1<-t(D) %*% D
H2<-solve(H1)
H2<-H2 %*% t(D)
theta_hat<-H2 %*% Y
theta_hat

reg<-lm(sqcase~Gpm)
summary(reg)
```

## Interpretation for Initial Models

Alright let me explain above really quickly.  When initially I stated thinking about the model, I was thinking of this more in terms of an autoregressive time series.  In generality, I wanted to derive a model from a general time series, $y_t=m_t+\epsilon_t$ where $m_t$ is the mean component given by the general regression model $m_t=\beta_0+\beta_1t_1+...+\beta_kt_k$ and $\epsilon_t$ is the autoregressive model given by $\epsilon_t=\phi_1\epsilon_{t-1}+\phi_2\epsilon_{t-2}+...+\phi_{t-p}\epsilon_{t-p}+z_t$. In particular, I was considering a first order autoregressive model given by $\epsilon_t=\phi_1\epsilon_{t-1}+z_t$.  That is where this first bit of output comes from, and we get a negative (albeit small in magnitude) slope.  However, then I started contemplating what this was actually emblematic of, and I am not convinced this is the right model (perhaps this was dumb to have done in the first place, but I got lured in seeing data that was time-stamped).  This is not considering the effect of our explanatory variable on our response, but treating our response as a function of time (duh these are variables that fluctuate with time, but time is not directly our explanatory variable).  Potentially useful information, but not what we are going for.  

So then, I just ran a simply linear regression of our root transformed explanatory variable on our response.  Here we get a positive slope coefficient (which is significant).  This slope signage is different than I would have hoped, but not entirely unsurprising given the plots of the data.  The main take-away at the moment is that our predictive capabilities (frankly) are piss power.  Obviously this is a first crude attempt at a model made after I went through time series contemplation, but including other variables is only going to do so much to bolster an adjusted R^2 of 0.024.  Not to worry (we can still mess around plenty with the model, and Dr. Smith ultimately capitulated that even if we have to admit that our anticipated results were not what we found, that is fine so long as the analysis was sound).  For now, let's just work on refining what we have.  

Let's try some other modeling methods
```{r}
### zero-inflate poisson glm

DF.Raleigh <- read.csv("derived_data/Raleigh.Clean.Covid.csv") %>% 
  select(utc, Location, PM25, Cases)
DF.Greenville <- read.csv("derived_data/Greenville.Clean.Covid.csv") %>% 
  select(utc, Location, PM25, Cases)

DF <- rbind(DF.Raleigh, DF.Greenville)

sum(DF.Raleigh$Cases == 0)
sum(DF.Greenville$Cases == 0)

head(DF)

library(pscl)

Ral.zero.mod <- zeroinfl(Cases ~ PM25, dist = "negbin", data = DF.Raleigh)
summary(Ral.zero.mod)

Green.zero.mod <- zeroinfl(Cases ~ PM25, dist = "negbin", data = DF.Greenville)
summary(Green.zero.mod)

source("Utils.R")

predictions <- predict(Green.zero.mod, type = "response")
Results <- eval_results(DF.Greenville$Cases, predictions, DF.Greenville)

#Let's make common glm models to see if the zeroinfl is at all better

Ral.glm <- glm(Cases ~ PM25, family = poisson, data = DF.Raleigh)
Green.glm <- glm(Cases ~ PM25, family = poisson, data = DF.Greenville)

predictions <- predict(Green.glm, type = "response")
Results <- eval_results(DF.Greenville$Cases, predictions, DF.Greenville)


#all of these are terrible, lets see what happens if we remove all the 0s from the sets and really check when corona started what happens. 

DF.Raleigh.noZ <- DF.Raleigh %>% 
  filter(Cases > 0)
DF.Green.noZ <- DF.Greenville %>% 
  filter(Cases > 0)

# now lets just try to make some glms

Ral.glm.noZ <- glm(Cases ~ PM25, family = poisson, data = DF.Raleigh.noZ)
Green.glm.noZ <- glm(Cases ~ PM25, family = poisson, data = DF.Green.noZ)
plot(DF.Raleigh.noZ$Cases)

summary(Ral.glm.noZ)
summary(Green.glm.noZ)

predictions <- predict(Ral.glm.noZ, type = "response")
Results <- eval_results(DF.Raleigh.noZ$Cases, predictions, DF.Raleigh.noZ)

predictions <- predict(Green.glm.noZ, type = "response")
Results <- eval_results(DF.Green.noZ$Cases, predictions, DF.Green.noZ)

#and some LMs without the 0s

Ral.lm.noZ <- lm(Cases ~ PM25, data = DF.Raleigh.noZ)
summary(Ral.lm.noZ)
bc.Ral <- boxcox(Ral.lm.noZ)
Ral.lam <- bc.Ral$x[which.max(bc.Ral$y)]

Ral.lm2.noZ <- lm(Cases^Ral.lam ~ PM25, data = DF.Raleigh.noZ)
summary(Ral.lm2.noZ)

Green.lm.noZ <- lm(Cases ~ PM25, data = DF.Green.noZ)
summary(Green.lm.noZ)
bc.green <- boxcox(Green.lm.noZ)
Green.lam <- bc.green$x[which.max(bc.green$y)]
Green.lm2.noZ <- lm(Cases^Green.lam ~ PM25, data = DF.Green.noZ)
summary(Green.lm2.noZ)
```

Well this didn't really lead to much. Tried the zero inflate and some normal glms. Also tried to just filter the data so it starts when the very first cases of Covid-19 come into play. This seemed to help a bit, all the 0s really mess with the model. 

Something sort of interesting did come out of this though. PM25 seems to be a significant predictor in the Raleigh set but not in the Greenvile set. There was also a negative beta estimate on the PM25 variable. This follows the hypothesis we suspected. In Greenville maybe it makes sense that there is not a significant relation between the two. If there laws were not as stringent there, wouldn't people have continued to go out and drive around? I'm sure plenty of people were breaking the stay at home order, maybe more did it in Greenville than in Raleigh. 

## Binaries included in model

```{r}
#filter data 
#Gdat.noZ <- filter(Gdat, Gcase>0)
#Rdat.noZ <- filter(Rdat, Rcase>0)
#square root transform
sqrtcase<-sqrt(Gcase)
sqrtCase<-sqrt(Rcase)

#stay at home binary 
#Greenville SAH ran from 4-7 to 5-4
#Raleigh SAH ran from 3-30 to 5-22
#unfortunately we are missing data from most of raleigh SAH
SAHG<-integer(length(t))
SAHR<-integer(length(d))
for (i in 88:114){
  SAHG[i]<-1
}
SAHG<-factor(SAHG)
SAHR[77:78]<-1
#Bar close mandate binary
#Bar
BG<-integer(length(t))
for (i in 81:114){
  BG[i]<-1
}
BG<-factor(BG)
BR<-integer(length(d))
for (i in 64:196){
  BR[i]<-1
}
BR<-factor(BR)
  


reg1<-lm(Gcase~Gpm+SAHG+BG)
summary(reg1)

reg2<-lm(Rcase~Rpm+BR)
summary(reg2)

```
##
Reflection

I believe flipping the model to now be an epidemiological analysis is more informative. In as much as particulate air matter includes particles commonly emitted from automobiles, industry exhaust, and active construction sites, this could be perceived as a proxy for "life per the status quo" or continuing to "go out" despite social distancing precaution. Greenville will yield our most intuitive interpretations, as Raleigh contains a hole in the data during the crucial stay at home period.  This likely puts undue influence on the latter case values.  However, for Greenville we see a highly significant and positive coefficient for particulate matter.  This would seem to indicate that as particulate air matter rises (i.e people continue to live life per their usual busy and intermingled lives) corona cases continue to increase.  The fact that Greenville experiences a significant positive coefficient while Raleigh has a negative (albeit starkly non-significant one) coincides with our intuition that Greenville's less stringent regulation and abidance results in more rapid spread.  It is also worth noting for Greenville that the bar closure mandate has a nearly significant and negative coefficient indicating that the introduction of bar closures results in lessened spread.  Such a regulation (state sanctioned bar closings) is more easily enforced that a broad stay at home order that leaves much to discretionary autonomy.  This disparity is reflected in Greenville's stay at home order being almost entirely indistinguishable from a zero effect.  Put another way, the stay at home order was fairly inconsequential; people did not adhere to it.  Taking a second to compare these results to Raleigh, it is again important to know that these results may be slightly skewed by missing data.  This is the best explanation I have been able to surmise regarding a significant positive bar closure binary.  It seems massively counterintutive that closing bars would be correlated with a massive and distinguishable spike in corona cases.  Now, it could be the case the left bereft of a regulated bar at which libations may be enjoyed people began having more unregulated, congregated house parties, and this resulted in heightened spread.  However, what seems far more likely is that the hole in the data corresponding to a stay at home order (and also a large portion of the time period corresponding to bar closures) places undue influence on binary 1's after the stay at home order had been lifted (bars were still closed) and people gradually began to naturally spread the virus again.  

Redo Andy's with Akshay Data
```{r}
DF <- read.csv("derived_data/DF.Final.No.Binary.csv")

#lets add binaries for stay at home and bars closing
DF = DF %>% 
  mutate(numerical_day = as.numeric(utc))
DF.full = DF %>% 
  mutate(Stay_At_Home = ifelse(DF$Location == "Greenville" & 
                                DF$numerical_day >= 85 & 
                                DF$numerical_day <= 111, 1, ifelse(DF$Location == "Raleigh" &
                                                                  DF$numerical_day >= 77 &
                                                                  DF$numerical_day <= 139, 1, 0))) %>% 
  mutate(Bar_Close = ifelse(DF$Location == "Greenville" &
                              DF$numerical_day >= 78 &
                              DF$numerical_day <= 111, 1, ifelse(DF$Location == "Raleigh" &
                                                                DF$numerical_day >= 66 &
                                                                DF$numerical_day <= 257, 1, 0))) %>% 
  select(-numerical_day)
DF.full$Stay_At_Home <- factor(DF.full$Stay_At_Home)
DF.full$Bar_Close <- factor(DF.full$Bar_Close)
```



Autocorrelation exploration and plots
```{r}
library(tidyverse)
Gdat<-read.csv("derived_data/Greenville.Clean.Covid.csv")
Rdat<-read.csv("derived_data/Raleigh.Clean.Covid.csv")
#initialize variables
t<-Gdat[,2]
gtime<-as.Date(t, "%Y-%m-%d")
Gpm<-Gdat[,5]
Gcase<-Gdat[,6]

d<-Rdat[,2]
rtime<-as.Date(d, "%Y-%m-%d")
Rpm<-Rdat[,9]
Rcase<-Rdat[,11]


GC.acf <- acf(Gcase)
G.acf.df <- with(GC.acf, data.frame(lag, acf))

g1 <- G.acf.df %>% 
  ggplot(aes(x = lag, y = acf)) +
  geom_hline(aes(yintercept = 0), color = "red") +
  geom_hline(aes(yintercept = .1), color = "blue", linetype = "dashed") +
  geom_hline(aes(yintercept = -.1), color = "blue", linetype = "dashed") +
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black")) +
  geom_segment(aes(xend = lag, yend = 0)) +
  xlab("Lag") +
  ylab("ACF") +
  ggtitle("Greenville Cases")
```

Matt Modeling with full DF

```{r}
library(tidyverse)
library(caret)
DF.Full <- read.csv("derived_data/DF.Final.csv") %>% 
  mutate(New_Cases = Cases - lag(Cases))
DF.Full$New_Cases[205] = 0
DF.Full$New_Cases[1] = 0
DF.Green <- DF.Full %>% 
  filter(Location == "Greenville") 
DF.Ral <- DF.Full %>% 
  filter(Location == "Raleigh")

# DF.Green$New_Cases <- scale(DF.Green$New_Cases)
# DF.Ral$New_Cases <- scale(DF.Ral$New_Cases)
# 
# DF.Full <- rbind(DF.Ral, DF.Green)
```

Model training
```{r}
set.seed = 18
trainIndex <- createDataPartition(y = DF.Full$New_Cases, p=.8, times = 1, list = F)

glm1 <- glm(New_Cases ~ O3 +
              PM25 +
              factor(Stay_At_Home) +
              factor(Bar_Close) +
              factor(Location) - 1, 
            data = DF.Full[trainIndex, ], 
            family = poisson)
summary(glm1)


plot(glm1)
source("Utils.R")

preds <- predict(glm1, newdata = DF.Full[-trainIndex, ], type = "response")
eval_results(DF.Full$New_Cases[-trainIndex], preds, DF.Full)

#we can try some interactions
glm2 <- glm(New_Cases ~ (O3 +
              PM25 +
              factor(Stay_At_Home) +
              factor(Bar_Close) +
              factor(Location))^2 - 1, 
            data = DF.Full[trainIndex, ], 
            family = poisson)
summary(glm2) #pretty tough to interpret, but i think location with binary interactions should be involved

glm3 <- glm(New_Cases ~ O3 +
              PM25 +
              factor(Stay_At_Home) +
              factor(Bar_Close) +
              factor(Location) +
              factor(Location)*factor(Bar_Close) +
              factor(Location)*factor(Stay_At_Home) - 1, 
            data = DF.Full[trainIndex, ], 
            family = poisson)
summary(glm3)
```

So, I tried to combine the variables you both went with into some models that were similar to Akshay, but they don't seem to have a great predictive power and some of the results I'm getting are a bit counterintuitive, I think largely due to the mising data in the Raleigh case. This is causing some outliers and for the stay at home variable for raleigh to just not be useful at all. I think I'll try separate models for for green and ral instead of using a location factor. The combination bar closing and loaction seemed like a resonable idea, but the raleigh data again had issues. 

I ended up building a new variable titled `New_Cases` which is just the number of cases - the number of cases the day before. I tried to normalize these using a centering but then I couldn't do a poisson glm for it. So I'm not going to do that, instead I'll try to use the newcasesper Akshay made. 